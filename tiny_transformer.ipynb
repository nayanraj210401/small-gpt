{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d321ecde",
   "metadata": {},
   "source": [
    "# Cell Descriptions\n",
    "1. **Install Dependencies**: Installs the required Python packages, including PyTorch and NumPy.\n",
    "2. **Data Preparation**: Prepares the text data for training by encoding characters into integers and creating input-output pairs.\n",
    "3. **Training Data Preparation**: Converts the input-output pairs into PyTorch tensors for model training.\n",
    "4. **Model Definition**: Defines the TinyGPT model, including its embedding and linear layers.\n",
    "5. **Training Loop**: Trains the TinyGPT model using the prepared data and prints the loss every 100 steps.\n",
    "6. **Text Generation Function**: Implements a function to generate text using the trained TinyGPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a12accfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.13/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.13/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.4-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Downloading numpy-2.2.4-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-2.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a93c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepartion of data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "text = \"\"\"The quick brown fox jumps over the lazy dog\"\"\"\n",
    "\n",
    "# Get all the unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Create mapping char --> index and index --> char\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "# Encoding: string to list of integers (tokens)\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "# Decoding: list of integers (tokens) to string\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87370be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prepare Training Data\n",
    "\n",
    "\n",
    "block_size = 8 # Number of input char the mode sees at a time\n",
    "\n",
    "X = [] # Input Sequence\n",
    "Y = [] # Target Sequence (next character for each input)\n",
    "\n",
    "\n",
    "# Loop over the text to create training example\n",
    "for i in range(len(text) - block_size):\n",
    "    chuck = text[i: i + block_size] # Input sequence\n",
    "    target = text[i+1: i + block_size + 1] # Target sequence shift by one char\n",
    "    # Convert in int\n",
    "    X.append(encode(chuck)) # Encode the input sequence\n",
    "    Y.append(encode(target)) # Encode the target sequence\n",
    "\n",
    "\n",
    "# Important to convert this into pytorch tensors\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "200a4cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build My Tiny GPT Model\n",
    "\n",
    "# NN\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed=32):\n",
    "        super().__init__()\n",
    "\n",
    "        # Learnable Embedding table that maps token id to vectors\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "\n",
    "        # Linear layer to project the embedding to vocab size  \n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx shape: (batch_size, block_size)\n",
    "\n",
    "        # Lookup embeddings for input tokens\n",
    "        embeddings = self.token_embedding_table(idx) # (batch_size, block_size, n_embed)\n",
    "\n",
    "        # Get the raw predictions for each position\n",
    "        logits = self.lm_head(embeddings)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None # For inference/generation only\n",
    "        \n",
    "        B, T, C = logits.shape # B: batch size, T: block size, C: vocab size\n",
    "        logits = logits.view(B * T, C)\n",
    "        targets = targets.view(B * T)\n",
    "\n",
    "        # Compute the loss between predictions and targets\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d45b89a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 3.5435\n",
      "Step 100, Loss: 1.8371\n",
      "Step 200, Loss: 1.0007\n",
      "Step 300, Loss: 0.7554\n",
      "Step 400, Loss: 0.6895\n"
     ]
    }
   ],
   "source": [
    "# 4. Train the Model\n",
    "\n",
    "# Instantiate the model\n",
    "model = TinyGPT(vocab_size)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for i in range(500):\n",
    "\n",
    "    # Forward pass\n",
    "    logits, loss = model(X, Y)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad() # Clear previous gradients\n",
    "    loss.backward() # Backpropagation\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step() # Update weights\n",
    "\n",
    "    # Print loss every 100 steps\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Step {i}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a71b8a",
   "metadata": {},
   "source": [
    "# Text Generation Function\n",
    "This cell defines a function `generate` that generates text using the trained TinyGPT model. The function takes a starting text and generates a sequence of tokens by sampling from the model's predictions. It uses the softmax function to convert logits into probabilities and samples the next token based on these probabilities. The generated tokens are then decoded back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34628ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abck ox jumps over juick fox doverox jumps own quickr fove own qumpazy quick broverove lazy tazy the oz\n"
     ]
    }
   ],
   "source": [
    "# 5. Text Generation Function\n",
    "\n",
    "def generate(model, start_text=\"Th \", max_new_tokens=100):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval();\n",
    "\n",
    "    # Convert start text to token idx\n",
    "    context = torch.tensor(\n",
    "        [encode(start_text)], dtype=torch.long\n",
    "    )\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Only use the last `block_size` tokens for prediction\n",
    "        logits, _ = model(context[:, -block_size:])\n",
    "\n",
    "        # Focus on the last tim steps logits\n",
    "        probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "        # Sample the next token from probability distribution\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Append the predicted token to the context\n",
    "        context = torch.cat((context, next_id), dim=1)\n",
    "    # Convert the token idx to string\n",
    "    return decode(context[0].tolist())\n",
    "\n",
    "\n",
    "print(generate(model, start_text=\"abc\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
